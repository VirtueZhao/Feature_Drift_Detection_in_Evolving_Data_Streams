{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import random\n",
    "import shap\n",
    "import lime\n",
    "import eli5\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Significant_Wasserstein_Distance(df_1, df_2):\n",
    "    significant_flag = False\n",
    "    threshold = 0.04\n",
    "    feature_detected = []\n",
    "    \n",
    "    feature_names = df_1.columns\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        df_1_feature_values = df_1[feature]\n",
    "        df_2_feature_values = df_2[feature]\n",
    "        \n",
    "        distance = stats.wasserstein_distance(df_1_feature_values, df_2_feature_values)\n",
    "\n",
    "        if distance > threshold:\n",
    "            print(\"Feature Name: \" + feature + \" Distance: \" + str(distance))\n",
    "            significant_flag = True\n",
    "            feature_detected.append(feature)\n",
    "        \n",
    "    return significant_flag, feature_detected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Check_Significant_Shap(model, reference_x, current_x):\n",
    "    print(\"Checking Significant by Shap values\")\n",
    "    ref_shap_values = shap.TreeExplainer(model).shap_values(reference_x)[1]\n",
    "    cur_shap_values = shap.TreeExplainer(model).shap_values(current_x)[1]\n",
    "    \n",
    "    num_instances = len(ref_shap_values)\n",
    "    num_features = len(ref_shap_values[0])\n",
    "    \n",
    "    ref_feature_shap_values = []\n",
    "    cur_feature_shap_values = []\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        ref_feature_shap_value = []\n",
    "        cur_feature_shap_value = []\n",
    "        for j in range(num_instances):\n",
    "            ref_feature_shap_value.append(ref_shap_values[j][i])\n",
    "            cur_feature_shap_value.append(cur_shap_values[j][i])\n",
    "        ref_feature_shap_values.append(ref_feature_shap_value)\n",
    "        cur_feature_shap_values.append(cur_feature_shap_value)\n",
    "   \n",
    "    update_flag = False\n",
    "    \n",
    "    feature_detected = []\n",
    "    for i in range(num_features):\n",
    "        ref_feature_shap_value = ref_feature_shap_values[i]\n",
    "        cur_feature_shap_value = cur_feature_shap_values[i]\n",
    "        \n",
    "        ref_mean = statistics.mean(ref_feature_shap_value)\n",
    "        ref_stdev = statistics.stdev(ref_feature_shap_value)\n",
    "        ref_size = len(ref_feature_shap_value)\n",
    "        \n",
    "        cur_mean = statistics.mean(cur_feature_shap_value)\n",
    "        cur_stdev = statistics.stdev(cur_feature_shap_value)\n",
    "        cur_size = len(cur_feature_shap_value)\n",
    "        \n",
    "        significant,p = Two_Sample_T_test(ref_mean, cur_mean, ref_stdev, cur_stdev, ref_size, cur_size)\n",
    "        \n",
    "        if significant:\n",
    "#             print(\"Shapley Values Drift Detected at Feature \" + str(i))\n",
    "#             print(\"P-value: \" + str(p))\n",
    "            update_flag = True\n",
    "            feature_detected.append(i)\n",
    "    return update_flag, feature_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Check_Significant_ELI5_Local(model, reference_window, current_window):\n",
    "    print(\"Checking Significant by ELI5 Local Explaination\")\n",
    "    reference_window = reference_window.loc[reference_window['10'].astype(float) == 1.0]\n",
    "    reference_window_x = reference_window.drop('10',axis=1)\n",
    "    ref_eli5_distribution = Get_ELI5_Explaination(model, reference_window_x)\n",
    "    \n",
    "    current_window = current_window.loc[current_window['10'].astype(float) == 1.0]\n",
    "    current_window_x = current_window.drop('10',axis=1)\n",
    "    cur_eli5_distribution = Get_ELI5_Explaination(model, current_window_x)\n",
    "    \n",
    "    update_flag = False\n",
    "    \n",
    "    feature_detected = []\n",
    "    for feature in ref_eli5_distribution:\n",
    "        if feature in cur_eli5_distribution:\n",
    "            reference_mean = ref_eli5_distribution[feature][0]\n",
    "            current_mean = cur_eli5_distribution[feature][0]\n",
    "            reference_stdev = ref_eli5_distribution[feature][1]\n",
    "            current_stdev = cur_eli5_distribution[feature][1]\n",
    "            reference_size = ref_eli5_distribution[feature][2]\n",
    "            current_size = cur_eli5_distribution[feature][2]\n",
    "                        \n",
    "            eli5_significant, p = Two_Sample_T_test(reference_mean,current_mean,reference_stdev,current_stdev,\n",
    "                                                               reference_size,current_size)\n",
    "                        \n",
    "            if eli5_significant:\n",
    "                print(\"Feature Name: \" + feature)\n",
    "                print(\"P-value: \" + str(p))\n",
    "                f = feature[1:]\n",
    "                feature_detected.append(f)\n",
    "                update_flag = True\n",
    "    return update_flag, feature_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Get_ELI5_Explaination(model, df):\n",
    "    values = df.astype(float).values\n",
    "    feature_data = {}\n",
    "    for value in values:\n",
    "        exp = eli5.explain_prediction(model, value)\n",
    "        exp = eli5.format_as_dict(exp)\n",
    "        weights = exp['targets'][0]['feature_weights']['pos']\n",
    "        \n",
    "        for i in range(len(weights)):\n",
    "            temp = weights[i]\n",
    "            feature = temp['feature']\n",
    "            weight = temp['weight']\n",
    "            if feature not in feature_data:\n",
    "                feature_weights = []\n",
    "                feature_weights.append(weight)\n",
    "                feature_data[feature] = feature_weights\n",
    "            else:\n",
    "                feature_weights = feature_data[feature]\n",
    "                feature_weights.append(weight)\n",
    "                feature_data[feature] = feature_weights\n",
    "    \n",
    "    feature_distribution = {}\n",
    "    for feature in feature_data:\n",
    "        feature_values = feature_data[feature]\n",
    "        feature_mean = statistics.mean(feature_values)\n",
    "        if len(feature_values) > 1:\n",
    "            feature_stdev = statistics.stdev(feature_values)\n",
    "        else:\n",
    "            feature_stdev = 0\n",
    "        feature_distribution[feature] = [feature_mean, feature_stdev, len(feature_values)]\n",
    "    return feature_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Check_Significant_Permutation_Importance(model, reference_window, current_window):\n",
    "    print(\"Checking Significant by Permutation Importance\")\n",
    "    reference_window_x = reference_window.drop('10',axis=1)\n",
    "    reference_window_y = reference_window['10']\n",
    "    ref_pi = eli5.sklearn.PermutationImportance(model,random_state=42).fit(reference_window_x, reference_window_y)\n",
    "    ref_means = ref_pi.feature_importances_\n",
    "    ref_stdevs = ref_pi.feature_importances_std_\n",
    "    ref_size = len(reference_window_y)\n",
    "\n",
    "    current_window_x = current_window.drop('10',axis=1)\n",
    "    current_window_y = current_window['10']\n",
    "    cur_pi = eli5.sklearn.PermutationImportance(model,random_state=42).fit(current_window_x, current_window_y)\n",
    "    cur_means = cur_pi.feature_importances_\n",
    "    cur_stdevs = cur_pi.feature_importances_std_\n",
    "    cur_size = len(current_window_y)\n",
    "    \n",
    "    update_flag = False\n",
    "    feature_detected = []\n",
    "    for i in range(len(ref_means)):\n",
    "        ref_mean = ref_means[i]\n",
    "        ref_stdev = ref_stdevs[i]\n",
    "        cur_mean = cur_means[i]\n",
    "        cur_stdev = cur_stdevs[i]\n",
    "        \n",
    "        pi_significant,p = Two_Sample_T_test(ref_mean, cur_mean, ref_stdev, cur_stdev, ref_size, cur_size)\n",
    "        if pi_significant:\n",
    "            print(\"Feature Name: \" + str(i))\n",
    "            print(\"P-value: \" + str(p))\n",
    "            update_flag = True\n",
    "            feature_detected.append(i)\n",
    "    return update_flag, feature_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def Two_Sample_T_test(expected_mean, current_mean, expected_sd, current_sd, expected_size, current_size):\n",
    "    mean_diff = expected_mean - current_mean\n",
    "    size_sum = math.pow(expected_sd, 2) / expected_size + math.pow(current_sd, 2) / current_size\n",
    "    if size_sum == 0:\n",
    "        size_sum = 0.0000000001\n",
    "    t = mean_diff / math.sqrt(size_sum)\n",
    "    df = expected_size + current_size - 2\n",
    "    p = (1 - stats.t.cdf(t, df=df)) * 2\n",
    "\n",
    "    if p < 0.0001:\n",
    "        return True, p\n",
    "    else:\n",
    "        return False, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Adult Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Evaluation_Iteration = 10\n",
    "Batch_Size = 2500\n",
    "Drift_Batch = 9\n",
    "Feature_Shift = [0,1,2,3,4,5,14,30,32,41,42,52,53,54,62,63]\n",
    "Feature_Not_Shift = [6,7,8,9,10,11,12,13,15,16,17,18,19,20,21,22,\n",
    "                     23,24,25,26,27,28,29,31,33,34,35,36,37,38,39,40,43,44,45,46,47,48,49,50,51,55,56,57,58,59,60,61,64]\n",
    "print(len(Feature_Shift))\n",
    "print(len(Feature_Not_Shift))\n",
    "\n",
    "\n",
    "TPR_Total = []\n",
    "FPR_Total = []\n",
    "Delay_Total = []\n",
    "Batch_9_Interpretation_TPR_Total = []\n",
    "Batch_9_Interpretation_FPR_Total = []\n",
    "Batch_10_Interpretation_TPR_Total = []\n",
    "Batch_10_Interpretation_FPR_Total = []\n",
    "\n",
    "for i in range(Evaluation_Iteration):\n",
    "    print(\"Currently Working on Iteration: \" + str(i+1))\n",
    "    \n",
    "    batch_index = 0\n",
    "    batch = []\n",
    "\n",
    "    reference_window = []\n",
    "    current_window = []\n",
    "\n",
    "    drifts_detected = []\n",
    "    eval_accuracy_no_change = []\n",
    "    eval_accuracy_distance = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    Delay = 0\n",
    "    Delay_Stop_Count_Flag = False\n",
    "    \n",
    "    with open('Datasets/Synthetic_Feature_Adult_Abrupt.csv') as csv_file:\n",
    "#     with open('Datasets/Synthetic_Feature_Adult_Gradual.csv') as csv_file:\n",
    "        drifts_detected = []\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            batch.append(row)\n",
    "            if len(batch) == Batch_Size:\n",
    "                if batch_index == 0:\n",
    "                    reference_window.extend(batch)\n",
    "                    reference_window_df = pd.DataFrame(reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('65', axis=1)\n",
    "                    reference_window_y = reference_window_df['65']\n",
    "                \n",
    "                    random_forest_no_change = RandomForestClassifier(n_estimators=20,random_state=42)\n",
    "                    random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                    random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                else:\n",
    "                    current_window.extend(batch)\n",
    "                    \n",
    "                    sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                    \n",
    "#                     print(len(reference_window))\n",
    "#                     print(len(sample_reference_window))\n",
    "                    \n",
    "                    reference_window_df = pd.DataFrame(sample_reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('65', axis=1)\n",
    "                    \n",
    "                    current_window_df = pd.DataFrame(current_window)\n",
    "                    current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                    current_window_x = current_window_df.drop('65', axis=1)\n",
    "                    current_window_y = current_window_df['65']\n",
    "                    \n",
    "                    pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                    accuracy_no_change = accuracy_score(current_window_y, pred_no_change)\n",
    "                    eval_accuracy_no_change.append(accuracy_no_change)\n",
    "                    \n",
    "                    pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                    accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                    eval_accuracy_distance.append(accuracy_distance)\n",
    "                    \n",
    "                    drift_flag, drift_feature = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                    \n",
    "                    if batch_index == Drift_Batch:\n",
    "                        if drift_flag:\n",
    "                            Delay = Batch_Size - 1921\n",
    "                            Delay_Stop_Count_Flag = True\n",
    "                        else:\n",
    "                            Delay = Batch_Size\n",
    "                    elif batch_index > Drift_Batch:\n",
    "                        if not(Delay_Stop_Count_Flag):\n",
    "                            if drift_flag:\n",
    "                                Delay = Delay + Batch_Size - 1921\n",
    "                                Delay_Stop_Count_Flag = True\n",
    "                            else:\n",
    "                                Delay + Batch_Size\n",
    "                    \n",
    "                    if drift_flag:\n",
    "                        print('Drift Detected at Batch ' + str(batch_index))\n",
    "                        \n",
    "#                         shap_drift_flag, feature_interpretation_shift_detected = Check_Significant_Shap(random_forest_distance, reference_window_x, current_window_x)\n",
    "#                         eli5_flag, feature_interpretation_shift_detected = Check_Significant_ELI5_Local(random_forest_distance, reference_window_df, current_window_df)\n",
    "                        PI_flag, feature_interpretation_shift_detected = Check_Significant_Permutation_Importance(random_forest_distance, reference_window_df, current_window_df)\n",
    "                        print(feature_interpretation_shift_detected)\n",
    "                        Interpretation_TP = 0\n",
    "                        Interpretation_FP = 0\n",
    "                        Interpretation_TN = 0\n",
    "                        Interpretation_FN = 0\n",
    "                        for feature in feature_interpretation_shift_detected:\n",
    "                            if feature in Feature_Shift:\n",
    "                                Interpretation_TP = Interpretation_TP + 1\n",
    "                            else:\n",
    "                                Interpretation_FP = Interpretation_FP + 1\n",
    "                        for feature in Feature_Shift:\n",
    "                            if feature not in feature_interpretation_shift_detected:\n",
    "                                Interpretation_FN = Interpretation_FN + 1\n",
    "                        for featurea in Feature_Not_Shift:\n",
    "                            if featurea not in feature_interpretation_shift_detected:\n",
    "                                Interpretation_TN = Interpretation_TN + 1\n",
    "#                         print(Interpretation_TP)\n",
    "#                         print(Interpretation_FN)\n",
    "#                         print(Interpretation_TN)\n",
    "#                         print(Interpretation_FP)\n",
    "                        Interpretation_TPR = Interpretation_TP / (Interpretation_TP + Interpretation_FN)\n",
    "                        Interpretation_FPR = Interpretation_FP / (Interpretation_FP + Interpretation_TN)\n",
    "#                         print(Interpretation_TPR)\n",
    "#                         print(Interpretation_FPR)\n",
    "                        if batch_index == 9:\n",
    "                            Batch_9_Interpretation_TPR_Total.append(Interpretation_TPR)\n",
    "                            Batch_9_Interpretation_FPR_Total.append(Interpretation_FPR)\n",
    "                        if batch_index == 10:\n",
    "                            Batch_10_Interpretation_TPR_Total.append(Interpretation_TPR)\n",
    "                            Batch_10_Interpretation_FPR_Total.append(Interpretation_FPR)\n",
    "                        \n",
    "                        \n",
    "                        drifts_detected.append(batch_index)\n",
    "                        random_forest_distance = RandomForestClassifier(n_estimators=20)\n",
    "                        random_forest_distance.fit(current_window_x, current_window_y)\n",
    "                        reference_window = []\n",
    "                        reference_window.extend(current_window)\n",
    "                        \n",
    "                        if batch_index == Drift_Batch:\n",
    "                            TP = TP + 1\n",
    "                        else:\n",
    "                            FP = FP + 1\n",
    "                    else:\n",
    "                        reference_window.extend(current_window)\n",
    "                        if batch_index == Drift_Batch:\n",
    "                            FN = FN + 1\n",
    "                        else:\n",
    "                            TN = TN + 1\n",
    "#                     reference_window = []\n",
    "#                     reference_window.extend(current_window)\n",
    "                    current_window = []\n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                    \n",
    "        TPR = TP / (TP + FN)\n",
    "        FPR = FP / (FP + TN)\n",
    "        TPR_Total.append(TPR)\n",
    "        FPR_Total.append(FPR)\n",
    "        Delay_Total.append(Delay)\n",
    "\n",
    "mean_TPR = statistics.mean(TPR_Total)\n",
    "stdev_TPR = statistics.stdev(TPR_Total)\n",
    "mean_FPR = statistics.mean(FPR_Total)\n",
    "stdev_FPR = statistics.stdev(FPR_Total)\n",
    "mean_Delay = statistics.mean(Delay_Total)\n",
    "stdev_Delay = statistics.stdev(Delay_Total)\n",
    "\n",
    "print(\"Feature Drift Detection\")\n",
    "print(\"TPR Total: \" + str(TPR_Total))\n",
    "print(\"FPR Total: \" + str(FPR_Total))\n",
    "print(\"Delay Total: \" + str(Delay_Total))\n",
    "print(\"TPR Mean: \" + str(mean_TPR))\n",
    "print(\"TPR Std: \" + str(stdev_TPR))\n",
    "print(\"FPR Mean: \" + str(mean_FPR))\n",
    "print(\"FPR Std: \" + str(stdev_FPR))\n",
    "print(\"Delay Mean: \" + str(mean_Delay))\n",
    "print(\"Delay Std: \" + str(stdev_Delay))\n",
    "\n",
    "print(\"Feature Drift Interpretation\")\n",
    "mean_Batch_9_Interpretation_TPR = statistics.mean(Batch_9_Interpretation_TPR_Total)\n",
    "stdev_Batch_9_Interpretation_TPR = statistics.stdev(Batch_9_Interpretation_TPR_Total)\n",
    "mean_Batch_9_Interpretation_FPR = statistics.mean(Batch_9_Interpretation_FPR_Total)\n",
    "stdev_Batch_9_Interpretation_FPR = statistics.stdev(Batch_9_Interpretation_FPR_Total)\n",
    "print(\"TPR Batch 9 Total: \" + str(Batch_9_Interpretation_TPR_Total))\n",
    "print(\"FPR Batch 9 Total: \" + str(Batch_9_Interpretation_FPR_Total))\n",
    "print(\"TPR Batch 9 Interpretation Mean: \" + str(mean_Batch_9_Interpretation_TPR))\n",
    "print(\"TPR Batch 9 Interpretation Std: \" + str(stdev_Batch_9_Interpretation_TPR))\n",
    "print(\"FPR Batch 9 Interpretation Mean: \" + str(mean_Batch_9_Interpretation_FPR))\n",
    "print(\"FPR Batch 9 Interpretation Std: \" + str(stdev_Batch_9_Interpretation_FPR))\n",
    "\n",
    "mean_Batch_10_Interpretation_TPR = statistics.mean(Batch_10_Interpretation_TPR_Total)\n",
    "stdev_Batch_10_Interpretation_TPR = statistics.stdev(Batch_10_Interpretation_TPR_Total)\n",
    "mean_Batch_10_Interpretation_FPR = statistics.mean(Batch_10_Interpretation_FPR_Total)\n",
    "stdev_Batch_10_Interpretation_FPR = statistics.stdev(Batch_10_Interpretation_FPR_Total)\n",
    "print(\"TPR Batch 10 Total: \" + str(Batch_10_Interpretation_TPR_Total))\n",
    "print(\"FPR Batch 10 Total: \" + str(Batch_10_Interpretation_FPR_Total))\n",
    "print(\"TPR Batch 10 Interpretation Mean: \" + str(mean_Batch_10_Interpretation_TPR))\n",
    "print(\"TPR Batch 10 Interpretation Std: \" + str(stdev_Batch_10_Interpretation_TPR))\n",
    "print(\"FPR Batch 10 Interpretation Mean: \" + str(mean_Batch_10_Interpretation_FPR))\n",
    "print(\"FPR Batch 10 Interpretation Std: \" + str(stdev_Batch_10_Interpretation_FPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Bank Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Evaluation_Iteration = 2\n",
    "Batch_Size = 2500\n",
    "Drift_Batch = 9\n",
    "Feature_Shift = [0,2,3,6,8,9,29,31,34,36,44,47]\n",
    "Feature_Not_Shift = [1,4,5,7,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,30,32,33,35,37,38,39,40,41,42,43,45,46]\n",
    "# print(len(Feature_Shift))\n",
    "# print(len(Feature_Not_Shift))\n",
    "\n",
    "TPR_Total = []\n",
    "FPR_Total = []\n",
    "Delay_Total = []\n",
    "Batch_9_Interpretation_TPR_Total = []\n",
    "Batch_9_Interpretation_FPR_Total = []\n",
    "\n",
    "\n",
    "for i in range(Evaluation_Iteration):\n",
    "    print(\"Currently Working on Iteration: \" + str(i+1))\n",
    "    \n",
    "    batch_index = 0\n",
    "    batch = []\n",
    "\n",
    "    reference_window = []\n",
    "    current_window = []\n",
    "\n",
    "    drifts_detected = []\n",
    "    eval_accuracy_no_change = []\n",
    "    eval_accuracy_distance = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    Delay = 0\n",
    "    Delay_Stop_Count_Flag = False\n",
    "    \n",
    "    with open('Datasets/Synthetic_Feature_Bank_Abrupt.csv') as csv_file:\n",
    "#     with open('Datasets/Synthetic_Feature_Bank_Gradual.csv') as csv_file:\n",
    "        drifts_detected = []\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            batch.append(row)\n",
    "            if len(batch) == Batch_Size:\n",
    "                print(\"Currently Working on Batch \" + str(batch_index))\n",
    "                if batch_index == 0:\n",
    "                    reference_window.extend(batch)\n",
    "                    reference_window_df = pd.DataFrame(reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('48', axis=1)\n",
    "                    reference_window_y = reference_window_df['48']\n",
    "                \n",
    "                    random_forest_no_change = RandomForestClassifier(n_estimators=20,random_state=42)\n",
    "                    random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                    random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                else:\n",
    "                    current_window.extend(batch)\n",
    "                    \n",
    "                    sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                    \n",
    "#                     print(len(reference_window))\n",
    "#                     print(len(sample_reference_window))\n",
    "                    \n",
    "                    reference_window_df = pd.DataFrame(sample_reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('48', axis=1)\n",
    "                    \n",
    "                    current_window_df = pd.DataFrame(current_window)\n",
    "                    current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                    current_window_x = current_window_df.drop('48', axis=1)\n",
    "                    current_window_y = current_window_df['48']\n",
    "                    \n",
    "                    pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                    accuracy_no_change = accuracy_score(current_window_y, pred_no_change)\n",
    "                    eval_accuracy_no_change.append(accuracy_no_change)\n",
    "                    \n",
    "                    pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                    accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                    eval_accuracy_distance.append(accuracy_distance)\n",
    "                    \n",
    "                    drift_flag, drift_feature = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                    \n",
    "                    if batch_index == Drift_Batch:\n",
    "                        if drift_flag:\n",
    "                            Delay = Batch_Size - 106\n",
    "                            Delay_Stop_Count_Flag = True\n",
    "                        else:\n",
    "                            Delay = Batch_Size\n",
    "                    elif batch_index > Drift_Batch:\n",
    "                        if not(Delay_Stop_Count_Flag):\n",
    "                            if drift_flag:\n",
    "                                Delay = Delay + Batch_Size - 106\n",
    "                                Delay_Stop_Count_Flag = True\n",
    "                            else:\n",
    "                                Delay + Batch_Size\n",
    "                    \n",
    "                    if drift_flag:\n",
    "                        print('Drift Detected at Batch ' + str(batch_index))\n",
    "                        \n",
    "#                         shap_drift_flag, feature_interpretation_shift_detected = Check_Significant_Shap(random_forest_distance, reference_window_x, current_window_x)\n",
    "#                         eli5_flag, feature_interpretation_shift_detected = Check_Significant_ELI5_Local(random_forest_distance, reference_window_df, current_window_df)\n",
    "#                         PI_flag, feature_interpretation_shift_detected = Check_Significant_Permutation_Importance(random_forest_distance, reference_window_df, current_window_df)\n",
    "#                         print(feature_interpretation_shift_detected)\n",
    "#                         Interpretation_TP = 0\n",
    "#                         Interpretation_FP = 0\n",
    "#                         Interpretation_TN = 0\n",
    "#                         Interpretation_FN = 0\n",
    "#                         for feature in feature_interpretation_shift_detected:\n",
    "#                             if feature in Feature_Shift:\n",
    "#                                 Interpretation_TP = Interpretation_TP + 1\n",
    "#                             else:\n",
    "#                                 Interpretation_FP = Interpretation_FP + 1\n",
    "#                         for feature in Feature_Shift:\n",
    "#                             if feature not in feature_interpretation_shift_detected:\n",
    "#                                 Interpretation_FN = Interpretation_FN + 1\n",
    "#                         for featurea in Feature_Not_Shift:\n",
    "#                             if featurea not in feature_interpretation_shift_detected:\n",
    "#                                 Interpretation_TN = Interpretation_TN + 1\n",
    "#                         print(Interpretation_TP)\n",
    "#                         print(Interpretation_FN)\n",
    "#                         print(Interpretation_TN)\n",
    "#                         print(Interpretation_FP)\n",
    "#                         Interpretation_TPR = Interpretation_TP / (Interpretation_TP + Interpretation_FN)\n",
    "#                         Interpretation_FPR = Interpretation_FP / (Interpretation_FP + Interpretation_TN)\n",
    "#                         print(Interpretation_TPR)\n",
    "#                         print(Interpretation_FPR)\n",
    "#                         if batch_index == 9:\n",
    "#                             Batch_9_Interpretation_TPR_Total.append(Interpretation_TPR)\n",
    "#                             Batch_9_Interpretation_FPR_Total.append(Interpretation_FPR)\n",
    "                        \n",
    "                        drifts_detected.append(batch_index)\n",
    "                        random_forest_distance = RandomForestClassifier(n_estimators=20)\n",
    "                        random_forest_distance.fit(current_window_x, current_window_y)\n",
    "                        reference_window = []\n",
    "                        reference_window.extend(current_window)\n",
    "                        \n",
    "                        if batch_index == Drift_Batch:\n",
    "                            TP = TP + 1\n",
    "                        else:\n",
    "                            FP = FP + 1\n",
    "                    else:\n",
    "                        reference_window.extend(current_window)\n",
    "                        if batch_index == Drift_Batch:\n",
    "                            FN = FN + 1\n",
    "                        else:\n",
    "                            TN = TN + 1\n",
    "#                     reference_window = []\n",
    "#                     reference_window.extend(current_window)\n",
    "                    current_window = []\n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                    \n",
    "        TPR = TP / (TP + FN)\n",
    "        FPR = FP / (FP + TN)\n",
    "        TPR_Total.append(TPR)\n",
    "        FPR_Total.append(FPR)\n",
    "        Delay_Total.append(Delay)\n",
    "\n",
    "mean_TPR = statistics.mean(TPR_Total)\n",
    "stdev_TPR = statistics.stdev(TPR_Total)\n",
    "mean_FPR = statistics.mean(FPR_Total)\n",
    "stdev_FPR = statistics.stdev(FPR_Total)\n",
    "mean_Delay = statistics.mean(Delay_Total)\n",
    "stdev_Delay = statistics.stdev(Delay_Total)\n",
    "\n",
    "print(\"TPR Total: \" + str(TPR_Total))\n",
    "print(\"FPR Total: \" + str(FPR_Total))\n",
    "print(\"Delay Total: \" + str(Delay_Total))\n",
    "print(\"TPR Mean: \" + str(mean_TPR))\n",
    "print(\"TPR Std: \" + str(stdev_TPR))\n",
    "print(\"FPR Mean: \" + str(mean_FPR))\n",
    "print(\"FPR Std: \" + str(stdev_FPR))\n",
    "print(\"Delay Mean: \" + str(mean_Delay))\n",
    "print(\"Delay Std: \" + str(stdev_Delay))\n",
    "\n",
    "# print(\"Feature Drift Interpretation\")\n",
    "# mean_Batch_9_Interpretation_TPR = statistics.mean(Batch_9_Interpretation_TPR_Total)\n",
    "# stdev_Batch_9_Interpretation_TPR = statistics.stdev(Batch_9_Interpretation_TPR_Total)\n",
    "# mean_Batch_9_Interpretation_FPR = statistics.mean(Batch_9_Interpretation_FPR_Total)\n",
    "# stdev_Batch_9_Interpretation_FPR = statistics.stdev(Batch_9_Interpretation_FPR_Total)\n",
    "# print(\"TPR Batch 9 Total: \" + str(Batch_9_Interpretation_TPR_Total))\n",
    "# print(\"FPR Batch 9 Total: \" + str(Batch_9_Interpretation_FPR_Total))\n",
    "# print(\"TPR Batch 9 Interpretation Mean: \" + str(mean_Batch_9_Interpretation_TPR))\n",
    "# print(\"TPR Batch 9 Interpretation Std: \" + str(stdev_Batch_9_Interpretation_TPR))\n",
    "# print(\"FPR Batch 9 Interpretation Mean: \" + str(mean_Batch_9_Interpretation_FPR))\n",
    "# print(\"FPR Batch 9 Interpretation Std: \" + str(stdev_Batch_9_Interpretation_FPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Credit Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Evaluation_Iteration = 10\n",
    "Batch_Size = 1500\n",
    "Drift_Batch = 10\n",
    "Feature_Shift = [4,5,6,7,8,9,16]\n",
    "Feature_Not_Shift = [0,1,2,3,10,11,12,13,14,15,17,18,19,20,21,22,23,24,25]\n",
    "print(len(Feature_Shift))\n",
    "print(len(Feature_Not_Shift))\n",
    "\n",
    "TPR_Total = []\n",
    "FPR_Total = []\n",
    "Delay_Total = []\n",
    "Batch_10_Interpretation_TPR_Total = []\n",
    "Batch_10_Interpretation_FPR_Total = []\n",
    "\n",
    "\n",
    "for i in range(Evaluation_Iteration):\n",
    "    print(\"Currently Working on Iteration: \" + str(i+1))\n",
    "    \n",
    "    batch_index = 0\n",
    "    batch = []\n",
    "\n",
    "    reference_window = []\n",
    "    current_window = []\n",
    "\n",
    "    drifts_detected = []\n",
    "    eval_accuracy_no_change = []\n",
    "    eval_accuracy_distance = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    Delay = 0\n",
    "    Delay_Stop_Count_Flag = False\n",
    "    \n",
    "    with open('Datasets/Synthetic_Feature_Credit_Abrupt.csv') as csv_file:\n",
    "#     with open('Datasets/Synthetic_Feature_Credit_Gradual.csv') as csv_file:\n",
    "        drifts_detected = []\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            batch.append(row)\n",
    "            if len(batch) == Batch_Size:\n",
    "                if batch_index == 0:\n",
    "                    reference_window.extend(batch)\n",
    "                    reference_window_df = pd.DataFrame(reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('26', axis=1)\n",
    "                    reference_window_y = reference_window_df['26']\n",
    "                \n",
    "                    random_forest_no_change = RandomForestClassifier(n_estimators=20,random_state=42)\n",
    "                    random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                    random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                else:\n",
    "                    current_window.extend(batch)\n",
    "                    \n",
    "                    sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                    \n",
    "#                     print(len(reference_window))\n",
    "#                     print(len(sample_reference_window))\n",
    "                    \n",
    "                    reference_window_df = pd.DataFrame(sample_reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('26', axis=1)\n",
    "                    \n",
    "                    current_window_df = pd.DataFrame(current_window)\n",
    "                    current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                    current_window_x = current_window_df.drop('26', axis=1)\n",
    "                    current_window_y = current_window_df['26']\n",
    "                    \n",
    "                    pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                    accuracy_no_change = accuracy_score(current_window_y, pred_no_change)\n",
    "                    eval_accuracy_no_change.append(accuracy_no_change)\n",
    "                    \n",
    "                    pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                    accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                    eval_accuracy_distance.append(accuracy_distance)\n",
    "                    \n",
    "                    drift_flag, drift_feature = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                    \n",
    "                    if batch_index == Drift_Batch:\n",
    "                        if drift_flag:\n",
    "                            Delay = Batch_Size - 1\n",
    "                            Delay_Stop_Count_Flag = True\n",
    "                        else:\n",
    "                            Delay = Batch_Size\n",
    "                    elif batch_index > Drift_Batch:\n",
    "                        if not(Delay_Stop_Count_Flag):\n",
    "                            if drift_flag:\n",
    "                                Delay = Delay + Batch_Size - 1\n",
    "                                Delay_Stop_Count_Flag = True\n",
    "                            else:\n",
    "                                Delay + Batch_Size\n",
    "                    \n",
    "                    if drift_flag:\n",
    "                        print('Drift Detected at Batch ' + str(batch_index))\n",
    "                        \n",
    "#                         shap_drift_flag, feature_interpretation_shift_detected = Check_Significant_Shap(random_forest_distance, reference_window_x, current_window_x)\n",
    "#                         eli5_flag, feature_interpretation_shift_detected = Check_Significant_ELI5_Local(random_forest_distance, reference_window_df, current_window_df)\n",
    "                        PI_flag, feature_interpretation_shift_detected = Check_Significant_Permutation_Importance(random_forest_distance, reference_window_df, current_window_df)\n",
    "                        print(feature_interpretation_shift_detected)\n",
    "                        Interpretation_TP = 0\n",
    "                        Interpretation_FP = 0\n",
    "                        Interpretation_TN = 0\n",
    "                        Interpretation_FN = 0\n",
    "                        for feature in feature_interpretation_shift_detected:\n",
    "                            if feature in Feature_Shift:\n",
    "                                Interpretation_TP = Interpretation_TP + 1\n",
    "                            else:\n",
    "                                Interpretation_FP = Interpretation_FP + 1\n",
    "                        for feature in Feature_Shift:\n",
    "                            if feature not in feature_interpretation_shift_detected:\n",
    "                                Interpretation_FN = Interpretation_FN + 1\n",
    "                        for featurea in Feature_Not_Shift:\n",
    "                            if featurea not in feature_interpretation_shift_detected:\n",
    "                                Interpretation_TN = Interpretation_TN + 1\n",
    "#                         print(Interpretation_TP)\n",
    "#                         print(Interpretation_FN)\n",
    "#                         print(Interpretation_TN)\n",
    "#                         print(Interpretation_FP)\n",
    "                        Interpretation_TPR = Interpretation_TP / (Interpretation_TP + Interpretation_FN)\n",
    "                        Interpretation_FPR = Interpretation_FP / (Interpretation_FP + Interpretation_TN)\n",
    "#                         print(Interpretation_TPR)\n",
    "#                         print(Interpretation_FPR)\n",
    "\n",
    "                        if batch_index == 10:\n",
    "                            Batch_10_Interpretation_TPR_Total.append(Interpretation_TPR)\n",
    "                            Batch_10_Interpretation_FPR_Total.append(Interpretation_FPR)\n",
    "                        \n",
    "                        drifts_detected.append(batch_index)\n",
    "                        random_forest_distance = RandomForestClassifier(n_estimators=20)\n",
    "                        random_forest_distance.fit(current_window_x, current_window_y)\n",
    "                        reference_window = []\n",
    "                        reference_window.extend(current_window)\n",
    "                        \n",
    "                        if batch_index == Drift_Batch:\n",
    "                            TP = TP + 1\n",
    "                        else:\n",
    "                            FP = FP + 1\n",
    "                    else:\n",
    "                        reference_window.extend(current_window)\n",
    "                        if batch_index == Drift_Batch:\n",
    "                            FN = FN + 1\n",
    "                        else:\n",
    "                            TN = TN + 1\n",
    "#                     reference_window = []\n",
    "#                     reference_window.extend(current_window)\n",
    "                    current_window = []\n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                    \n",
    "        TPR = TP / (TP + FN)\n",
    "        FPR = FP / (FP + TN)\n",
    "        TPR_Total.append(TPR)\n",
    "        FPR_Total.append(FPR)\n",
    "        Delay_Total.append(Delay)\n",
    "\n",
    "mean_TPR = statistics.mean(TPR_Total)\n",
    "stdev_TPR = statistics.stdev(TPR_Total)\n",
    "mean_FPR = statistics.mean(FPR_Total)\n",
    "stdev_FPR = statistics.stdev(FPR_Total)\n",
    "mean_Delay = statistics.mean(Delay_Total)\n",
    "stdev_Delay = statistics.stdev(Delay_Total)\n",
    "\n",
    "print(\"TPR Total: \" + str(TPR_Total))\n",
    "print(\"FPR Total: \" + str(FPR_Total))\n",
    "print(\"Delay Total: \" + str(Delay_Total))\n",
    "print(\"TPR Mean: \" + str(mean_TPR))\n",
    "print(\"TPR Std: \" + str(stdev_TPR))\n",
    "print(\"FPR Mean: \" + str(mean_FPR))\n",
    "print(\"FPR Std: \" + str(stdev_FPR))\n",
    "print(\"Delay Mean: \" + str(mean_Delay))\n",
    "print(\"Delay Std: \" + str(stdev_Delay))\n",
    "\n",
    "mean_Batch_10_Interpretation_TPR = statistics.mean(Batch_10_Interpretation_TPR_Total)\n",
    "stdev_Batch_10_Interpretation_TPR = statistics.stdev(Batch_10_Interpretation_TPR_Total)\n",
    "mean_Batch_10_Interpretation_FPR = statistics.mean(Batch_10_Interpretation_FPR_Total)\n",
    "stdev_Batch_10_Interpretation_FPR = statistics.stdev(Batch_10_Interpretation_FPR_Total)\n",
    "print(\"TPR Batch 10 Total: \" + str(Batch_10_Interpretation_TPR_Total))\n",
    "print(\"FPR Batch 10 Total: \" + str(Batch_10_Interpretation_FPR_Total))\n",
    "print(\"TPR Batch 10 Interpretation Mean: \" + str(mean_Batch_10_Interpretation_TPR))\n",
    "print(\"TPR Batch 10 Interpretation Std: \" + str(stdev_Batch_10_Interpretation_TPR))\n",
    "print(\"FPR Batch 10 Interpretation Mean: \" + str(mean_Batch_10_Interpretation_FPR))\n",
    "print(\"FPR Batch 10 Interpretation Std: \" + str(stdev_Batch_10_Interpretation_FPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gamma Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Evaluation_Iteration = 10\n",
    "Batch_Size = 1000\n",
    "Drift_Batch = 9\n",
    "Feature_Shift = [0,1,8]\n",
    "Feature_Not_Shift = [2,3,4,5,6,7,9]\n",
    "print(len(Feature_Shift))\n",
    "print(len(Feature_Not_Shift))\n",
    "\n",
    "\n",
    "\n",
    "TPR_Total = []\n",
    "FPR_Total = []\n",
    "Delay_Total = []\n",
    "Batch_9_Interpretation_TPR_Total = []\n",
    "Batch_9_Interpretation_FPR_Total = []\n",
    "Batch_10_Interpretation_TPR_Total = []\n",
    "Batch_10_Interpretation_FPR_Total = []\n",
    "\n",
    "for i in range(Evaluation_Iteration):\n",
    "    print(\"Currently Working on Iteration: \" + str(i+1))\n",
    "    \n",
    "    batch_index = 0\n",
    "    batch = []\n",
    "\n",
    "    reference_window = []\n",
    "    current_window = []\n",
    "\n",
    "    drifts_detected = []\n",
    "    eval_accuracy_no_change = []\n",
    "    eval_accuracy_distance = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    Delay = 0\n",
    "    Delay_Stop_Count_Flag = False\n",
    "    \n",
    "    with open('Datasets/Synthetic_Feature_Gamma_Abrupt.csv') as csv_file:\n",
    "#     with open('Datasets/Synthetic_Feature_Gamma_Gradual.csv') as csv_file:\n",
    "        drifts_detected = []\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            batch.append(row)\n",
    "            if len(batch) == Batch_Size:\n",
    "                if batch_index == 0:\n",
    "                    reference_window.extend(batch)\n",
    "                    reference_window_df = pd.DataFrame(reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('10', axis=1)\n",
    "                    reference_window_y = reference_window_df['10']\n",
    "                \n",
    "                    random_forest_no_change = RandomForestClassifier(n_estimators=20,random_state=42)\n",
    "                    random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                    random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                else:\n",
    "                    current_window.extend(batch)\n",
    "                    \n",
    "                    sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                    \n",
    "#                     print(len(reference_window))\n",
    "#                     print(len(sample_reference_window))\n",
    "                    \n",
    "                    reference_window_df = pd.DataFrame(sample_reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('10', axis=1)\n",
    "                    \n",
    "                    current_window_df = pd.DataFrame(current_window)\n",
    "                    current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                    current_window_x = current_window_df.drop('10', axis=1)\n",
    "                    current_window_y = current_window_df['10']\n",
    "                    \n",
    "                    pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                    accuracy_no_change = accuracy_score(current_window_y, pred_no_change)\n",
    "                    eval_accuracy_no_change.append(accuracy_no_change)\n",
    "                    \n",
    "                    pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                    accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                    eval_accuracy_distance.append(accuracy_distance)\n",
    "                    \n",
    "                    drift_flag, drift_feature = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                    \n",
    "                    if batch_index == Drift_Batch:\n",
    "                        if drift_flag:\n",
    "                            Delay = Batch_Size - 510\n",
    "                            Delay_Stop_Count_Flag = True\n",
    "                        else:\n",
    "                            Delay = Batch_Size\n",
    "                    elif batch_index > Drift_Batch:\n",
    "                        if not(Delay_Stop_Count_Flag):\n",
    "                            if drift_flag:\n",
    "                                Delay = Delay + Batch_Size - 510\n",
    "                                Delay_Stop_Count_Flag = True\n",
    "                            else:\n",
    "                                Delay + Batch_Size\n",
    "                    \n",
    "                    if drift_flag:\n",
    "                        print('Drift Detected at Batch ' + str(batch_index))\n",
    "                        \n",
    "#                         shap_drift_flag, feature_interpretation_shift_detected = Check_Significant_Shap(random_forest_distance, reference_window_x, current_window_x)\n",
    "#                         eli5_flag, feature_interpretation_shift_detected = Check_Significant_ELI5_Local(random_forest_distance, reference_window_df, current_window_df)\n",
    "                        PI_flag, feature_interpretation_shift_detected = Check_Significant_Permutation_Importance(random_forest_distance, reference_window_df, current_window_df)\n",
    "                        print(feature_interpretation_shift_detected)\n",
    "                        Interpretation_TP = 0\n",
    "                        Interpretation_FP = 0\n",
    "                        Interpretation_TN = 0\n",
    "                        Interpretation_FN = 0\n",
    "                        for feature in feature_interpretation_shift_detected:\n",
    "                            if feature in Feature_Shift:\n",
    "                                Interpretation_TP = Interpretation_TP + 1\n",
    "                            else:\n",
    "                                Interpretation_FP = Interpretation_FP + 1\n",
    "                        for feature in Feature_Shift:\n",
    "                            if feature not in feature_interpretation_shift_detected:\n",
    "                                Interpretation_FN = Interpretation_FN + 1\n",
    "                        for featurea in Feature_Not_Shift:\n",
    "                            if featurea not in feature_interpretation_shift_detected:\n",
    "                                Interpretation_TN = Interpretation_TN + 1\n",
    "#                         print(Interpretation_TP)\n",
    "#                         print(Interpretation_FN)\n",
    "#                         print(Interpretation_TN)\n",
    "#                         print(Interpretation_FP)\n",
    "                        Interpretation_TPR = Interpretation_TP / (Interpretation_TP + Interpretation_FN)\n",
    "                        Interpretation_FPR = Interpretation_FP / (Interpretation_FP + Interpretation_TN)\n",
    "#                         print(Interpretation_TPR)\n",
    "#                         print(Interpretation_FPR)\n",
    "                        if batch_index == 9:\n",
    "                            Batch_9_Interpretation_TPR_Total.append(Interpretation_TPR)\n",
    "                            Batch_9_Interpretation_FPR_Total.append(Interpretation_FPR)\n",
    "                        if batch_index == 10:\n",
    "                            Batch_10_Interpretation_TPR_Total.append(Interpretation_TPR)\n",
    "                            Batch_10_Interpretation_FPR_Total.append(Interpretation_FPR)\n",
    "                        \n",
    "                        drifts_detected.append(batch_index)\n",
    "                        random_forest_distance = RandomForestClassifier(n_estimators=20)\n",
    "                        random_forest_distance.fit(current_window_x, current_window_y)\n",
    "                        reference_window = []\n",
    "                        reference_window.extend(current_window)\n",
    "                        \n",
    "                        if batch_index == Drift_Batch:\n",
    "                            TP = TP + 1\n",
    "                        else:\n",
    "                            FP = FP + 1\n",
    "                    else:\n",
    "                        reference_window.extend(current_window)\n",
    "                        if batch_index == Drift_Batch:\n",
    "                            FN = FN + 1\n",
    "                        else:\n",
    "                            TN = TN + 1\n",
    "#                     reference_window = []\n",
    "#                     reference_window.extend(current_window)\n",
    "                    current_window = []\n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                    \n",
    "        TPR = TP / (TP + FN)\n",
    "        FPR = FP / (FP + TN)\n",
    "        TPR_Total.append(TPR)\n",
    "        FPR_Total.append(FPR)\n",
    "        Delay_Total.append(Delay)\n",
    "\n",
    "mean_TPR = statistics.mean(TPR_Total)\n",
    "stdev_TPR = statistics.stdev(TPR_Total)\n",
    "mean_FPR = statistics.mean(FPR_Total)\n",
    "stdev_FPR = statistics.stdev(FPR_Total)\n",
    "mean_Delay = statistics.mean(Delay_Total)\n",
    "stdev_Delay = statistics.stdev(Delay_Total)\n",
    "\n",
    "print(\"TPR Total: \" + str(TPR_Total))\n",
    "print(\"FPR Total: \" + str(FPR_Total))\n",
    "print(\"Delay Total: \" + str(Delay_Total))\n",
    "print(\"TPR Mean: \" + str(mean_TPR))\n",
    "print(\"TPR Std: \" + str(stdev_TPR))\n",
    "print(\"FPR Mean: \" + str(mean_FPR))\n",
    "print(\"FPR Std: \" + str(stdev_FPR))\n",
    "print(\"Delay Mean: \" + str(mean_Delay))\n",
    "print(\"Delay Std: \" + str(stdev_Delay))\n",
    "\n",
    "print(\"Feature Drift Interpretation\")\n",
    "mean_Batch_9_Interpretation_TPR = statistics.mean(Batch_9_Interpretation_TPR_Total)\n",
    "stdev_Batch_9_Interpretation_TPR = statistics.stdev(Batch_9_Interpretation_TPR_Total)\n",
    "mean_Batch_9_Interpretation_FPR = statistics.mean(Batch_9_Interpretation_FPR_Total)\n",
    "stdev_Batch_9_Interpretation_FPR = statistics.stdev(Batch_9_Interpretation_FPR_Total)\n",
    "print(\"TPR Batch 9 Total: \" + str(Batch_9_Interpretation_TPR_Total))\n",
    "print(\"FPR Batch 9 Total: \" + str(Batch_9_Interpretation_FPR_Total))\n",
    "print(\"TPR Batch 9 Interpretation Mean: \" + str(mean_Batch_9_Interpretation_TPR))\n",
    "print(\"TPR Batch 9 Interpretation Std: \" + str(stdev_Batch_9_Interpretation_TPR))\n",
    "print(\"FPR Batch 9 Interpretation Mean: \" + str(mean_Batch_9_Interpretation_FPR))\n",
    "print(\"FPR Batch 9 Interpretation Std: \" + str(stdev_Batch_9_Interpretation_FPR))\n",
    "\n",
    "mean_Batch_10_Interpretation_TPR = statistics.mean(Batch_10_Interpretation_TPR_Total)\n",
    "stdev_Batch_10_Interpretation_TPR = statistics.stdev(Batch_10_Interpretation_TPR_Total)\n",
    "mean_Batch_10_Interpretation_FPR = statistics.mean(Batch_10_Interpretation_FPR_Total)\n",
    "stdev_Batch_10_Interpretation_FPR = statistics.stdev(Batch_10_Interpretation_FPR_Total)\n",
    "print(\"TPR Batch 10 Total: \" + str(Batch_10_Interpretation_TPR_Total))\n",
    "print(\"FPR Batch 10 Total: \" + str(Batch_10_Interpretation_FPR_Total))\n",
    "print(\"TPR Batch 10 Interpretation Mean: \" + str(mean_Batch_10_Interpretation_TPR))\n",
    "print(\"TPR Batch 10 Interpretation Std: \" + str(stdev_Batch_10_Interpretation_TPR))\n",
    "print(\"FPR Batch 10 Interpretation Mean: \" + str(mean_Batch_10_Interpretation_FPR))\n",
    "print(\"FPR Batch 10 Interpretation Std: \" + str(stdev_Batch_10_Interpretation_FPR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# SEA Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Evaluation_Iteration = 50\n",
    "Batch_Size = 2500\n",
    "Drift_Batch = 20\n",
    "\n",
    "TPR_Total = []\n",
    "FPR_Total = []\n",
    "Delay_Total = []\n",
    "\n",
    "for i in range(Evaluation_Iteration):\n",
    "    print(\"Currently Working on Iteration: \" + str(i+1))\n",
    "    \n",
    "    batch_index = 0\n",
    "    batch = []\n",
    "\n",
    "    reference_window = []\n",
    "    current_window = []\n",
    "\n",
    "    drifts_detected = []\n",
    "    eval_accuracy_no_change = []\n",
    "    eval_accuracy_distance = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    Delay = 0\n",
    "    Delay_Stop_Count_Flag = False\n",
    "    \n",
    "    with open('Datasets/Synthetic_MOA_SEA_Abrupt.csv') as csv_file:\n",
    "#     with open('Datasets/Synthetic_MOA_RBF_Gradual.csv') as csv_file:\n",
    "        drifts_detected = []\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            batch.append(row)\n",
    "            if len(batch) == Batch_Size:\n",
    "                if batch_index == 0:\n",
    "                    reference_window.extend(batch)\n",
    "                    reference_window_df = pd.DataFrame(reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('3', axis=1)\n",
    "                    reference_window_y = reference_window_df['3']\n",
    "                \n",
    "                    random_forest_no_change = RandomForestClassifier(n_estimators=20,random_state=42)\n",
    "                    random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                    random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                else:\n",
    "                    current_window.extend(batch)\n",
    "                    \n",
    "                    sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                    \n",
    "#                     print(len(reference_window))\n",
    "#                     print(len(sample_reference_window))\n",
    "                    \n",
    "                    reference_window_df = pd.DataFrame(sample_reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('3', axis=1)\n",
    "                    \n",
    "                    current_window_df = pd.DataFrame(current_window)\n",
    "                    current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                    current_window_x = current_window_df.drop('3', axis=1)\n",
    "                    current_window_y = current_window_df['3']\n",
    "                    \n",
    "                    pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                    accuracy_no_change = accuracy_score(current_window_y, pred_no_change)\n",
    "                    eval_accuracy_no_change.append(accuracy_no_change)\n",
    "                    \n",
    "                    pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                    accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                    eval_accuracy_distance.append(accuracy_distance)\n",
    "                    \n",
    "                    drift_flag, drift_feature = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                    \n",
    "                    if batch_index == Drift_Batch:\n",
    "                        if drift_flag:\n",
    "                            Delay = Batch_Size - 1\n",
    "                            Delay_Stop_Count_Flag = True\n",
    "                        else:\n",
    "                            Delay = Batch_Size\n",
    "                    elif batch_index > Drift_Batch:\n",
    "                        if not(Delay_Stop_Count_Flag):\n",
    "                            if drift_flag:\n",
    "                                Delay = Delay + Batch_Size - 1\n",
    "                                Delay_Stop_Count_Flag = True\n",
    "                            else:\n",
    "                                Delay + Batch_Size\n",
    "                    \n",
    "                    if drift_flag:\n",
    "                        print('Drift Detected at Batch ' + str(batch_index))\n",
    "                        drifts_detected.append(batch_index)\n",
    "                        random_forest_distance = RandomForestClassifier(n_estimators=20)\n",
    "                        random_forest_distance.fit(current_window_x, current_window_y)\n",
    "                        reference_window = []\n",
    "                        reference_window.extend(current_window)\n",
    "                        \n",
    "                        if batch_index == Drift_Batch:\n",
    "                            TP = TP + 1\n",
    "                        else:\n",
    "                            FP = FP + 1\n",
    "                    else:\n",
    "                        reference_window.extend(current_window)\n",
    "                        if batch_index == Drift_Batch:\n",
    "                            FN = FN + 1\n",
    "                        else:\n",
    "                            TN = TN + 1\n",
    "#                     reference_window = []\n",
    "#                     reference_window.extend(current_window)\n",
    "                    current_window = []\n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                    \n",
    "        TPR = TP / (TP + FN)\n",
    "        FPR = FP / (FP + TN)\n",
    "        TPR_Total.append(TPR)\n",
    "        FPR_Total.append(FPR)\n",
    "        Delay_Total.append(Delay)\n",
    "\n",
    "mean_TPR = statistics.mean(TPR_Total)\n",
    "stdev_TPR = statistics.stdev(TPR_Total)\n",
    "mean_FPR = statistics.mean(FPR_Total)\n",
    "stdev_FPR = statistics.stdev(FPR_Total)\n",
    "mean_Delay = statistics.mean(Delay_Total)\n",
    "stdev_Delay = statistics.stdev(Delay_Total)\n",
    "\n",
    "print(\"TPR Total: \" + str(TPR_Total))\n",
    "print(\"FPR Total: \" + str(FPR_Total))\n",
    "print(\"Delay Total: \" + str(Delay_Total))\n",
    "print(\"TPR Mean: \" + str(mean_TPR))\n",
    "print(\"TPR Std: \" + str(stdev_TPR))\n",
    "print(\"FPR Mean: \" + str(mean_FPR))\n",
    "print(\"FPR Std: \" + str(stdev_FPR))\n",
    "print(\"Delay Mean: \" + str(mean_Delay))\n",
    "print(\"Delay Std: \" + str(stdev_Delay))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# RBF Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Evaluation_Iteration = 50\n",
    "Batch_Size = 2500\n",
    "Drift_Batch = 20\n",
    "\n",
    "TPR_Total = []\n",
    "FPR_Total = []\n",
    "Delay_Total = []\n",
    "\n",
    "for i in range(Evaluation_Iteration):\n",
    "    print(\"Currently Working on Iteration: \" + str(i+1))\n",
    "    \n",
    "    batch_index = 0\n",
    "    batch = []\n",
    "\n",
    "    reference_window = []\n",
    "    current_window = []\n",
    "\n",
    "    drifts_detected = []\n",
    "    eval_accuracy_no_change = []\n",
    "    eval_accuracy_distance = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    Delay = 0\n",
    "    Delay_Stop_Count_Flag = False\n",
    "    \n",
    "#     with open('Datasets/Synthetic_MOA_RBF_Abrupt.csv') as csv_file:\n",
    "    with open('Datasets/Synthetic_MOA_RBF_Gradual.csv') as csv_file:\n",
    "        drifts_detected = []\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            batch.append(row)\n",
    "            if len(batch) == Batch_Size:\n",
    "                if batch_index == 0:\n",
    "                    reference_window.extend(batch)\n",
    "                    reference_window_df = pd.DataFrame(reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('10', axis=1)\n",
    "                    reference_window_y = reference_window_df['10']\n",
    "                \n",
    "                    random_forest_no_change = RandomForestClassifier(n_estimators=20,random_state=42)\n",
    "                    random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                    random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                else:\n",
    "                    current_window.extend(batch)\n",
    "                    \n",
    "                    sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                    \n",
    "#                     print(len(reference_window))\n",
    "#                     print(len(sample_reference_window))\n",
    "                    \n",
    "                    reference_window_df = pd.DataFrame(sample_reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('10', axis=1)\n",
    "                    \n",
    "                    current_window_df = pd.DataFrame(current_window)\n",
    "                    current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                    current_window_x = current_window_df.drop('10', axis=1)\n",
    "                    current_window_y = current_window_df['10']\n",
    "                    \n",
    "                    pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                    accuracy_no_change = accuracy_score(current_window_y, pred_no_change)\n",
    "                    eval_accuracy_no_change.append(accuracy_no_change)\n",
    "                    \n",
    "                    pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                    accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                    eval_accuracy_distance.append(accuracy_distance)\n",
    "                    \n",
    "                    drift_flag, drift_feature = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                    \n",
    "                    if batch_index == Drift_Batch:\n",
    "                        if drift_flag:\n",
    "                            Delay = Batch_Size - 1\n",
    "                            Delay_Stop_Count_Flag = True\n",
    "                        else:\n",
    "                            Delay = Batch_Size\n",
    "                    elif batch_index > Drift_Batch:\n",
    "                        if not(Delay_Stop_Count_Flag):\n",
    "                            if drift_flag:\n",
    "                                Delay = Delay + Batch_Size - 1\n",
    "                                Delay_Stop_Count_Flag = True\n",
    "                            else:\n",
    "                                Delay + Batch_Size\n",
    "                    \n",
    "                    if drift_flag:\n",
    "                        print('Drift Detected at Batch ' + str(batch_index))\n",
    "                        drifts_detected.append(batch_index)\n",
    "                        random_forest_distance = RandomForestClassifier(n_estimators=20)\n",
    "                        random_forest_distance.fit(current_window_x, current_window_y)\n",
    "                        reference_window = []\n",
    "                        reference_window.extend(current_window)\n",
    "                        \n",
    "                        if batch_index == Drift_Batch:\n",
    "                            TP = TP + 1\n",
    "                        else:\n",
    "                            FP = FP + 1\n",
    "                    else:\n",
    "                        reference_window.extend(current_window)\n",
    "                        if batch_index == Drift_Batch:\n",
    "                            FN = FN + 1\n",
    "                        else:\n",
    "                            TN = TN + 1\n",
    "#                     reference_window = []\n",
    "#                     reference_window.extend(current_window)\n",
    "                    current_window = []\n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                    \n",
    "        TPR = TP / (TP + FN)\n",
    "        FPR = FP / (FP + TN)\n",
    "        TPR_Total.append(TPR)\n",
    "        FPR_Total.append(FPR)\n",
    "        Delay_Total.append(Delay)\n",
    "\n",
    "mean_TPR = statistics.mean(TPR_Total)\n",
    "stdev_TPR = statistics.stdev(TPR_Total)\n",
    "mean_FPR = statistics.mean(FPR_Total)\n",
    "stdev_FPR = statistics.stdev(FPR_Total)\n",
    "mean_Delay = statistics.mean(Delay_Total)\n",
    "stdev_Delay = statistics.stdev(Delay_Total)\n",
    "\n",
    "print(\"TPR Total: \" + str(TPR_Total))\n",
    "print(\"FPR Total: \" + str(FPR_Total))\n",
    "print(\"Delay Total: \" + str(Delay_Total))\n",
    "print(\"TPR Mean: \" + str(mean_TPR))\n",
    "print(\"TPR Std: \" + str(stdev_TPR))\n",
    "print(\"FPR Mean: \" + str(mean_FPR))\n",
    "print(\"FPR Std: \" + str(stdev_FPR))\n",
    "print(\"Delay Mean: \" + str(mean_Delay))\n",
    "print(\"Delay Std: \" + str(stdev_Delay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Batch_Size = 2500\n",
    "batch_index = 0\n",
    "batch = []\n",
    "batches = []\n",
    "\n",
    "reference_window = []\n",
    "current_window = []\n",
    "\n",
    "drifts_detected = []\n",
    "eval_accuracy_no_change = []\n",
    "eval_accuracy_distance = []\n",
    "    \n",
    "with open('Datasets/Synthetic_MOA_RBF_Abrupt.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        batch.append(row)\n",
    "        if len(batch) == Batch_Size:\n",
    "            batches.append(batch_index)\n",
    "            print(\"Currently Working on Batch \" + str(batch_index))\n",
    "            if batch_index == 0:\n",
    "                reference_window.extend(batch)\n",
    "                reference_window_df = pd.DataFrame(reference_window)\n",
    "                reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                reference_window_x = reference_window_df.drop('10', axis=1)\n",
    "                reference_window_y = reference_window_df['10']\n",
    "                \n",
    "                random_forest_no_change = RandomForestClassifier(n_estimators=20,random_state=42)\n",
    "                random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                \n",
    "                pred_no_change = random_forest_no_change.predict(reference_window_x)\n",
    "                accuracy_no_change = accuracy_score(reference_window_y, pred_no_change)\n",
    "                eval_accuracy_no_change.append(accuracy_no_change)\n",
    "                    \n",
    "                pred_distance = random_forest_distance.predict(reference_window_x)\n",
    "                accuracy_distance = accuracy_score(reference_window_y, pred_distance)\n",
    "                eval_accuracy_distance.append(accuracy_distance)\n",
    "                \n",
    "                    \n",
    "                batch = []\n",
    "                batch_index = batch_index + 1\n",
    "            else:\n",
    "                current_window.extend(batch)\n",
    "                sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                    \n",
    "                reference_window_df = pd.DataFrame(sample_reference_window)\n",
    "                reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                reference_window_x = reference_window_df.drop('10', axis=1)\n",
    "                    \n",
    "                current_window_df = pd.DataFrame(current_window)\n",
    "                current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                current_window_x = current_window_df.drop('10', axis=1)\n",
    "                current_window_y = current_window_df['10']\n",
    "                    \n",
    "                pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                accuracy_no_change = accuracy_score(current_window_y, pred_no_change)\n",
    "                eval_accuracy_no_change.append(accuracy_no_change)\n",
    "                    \n",
    "                pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                eval_accuracy_distance.append(accuracy_distance)\n",
    "                    \n",
    "                drift_flag, drift_feature = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                    \n",
    "                if drift_flag:\n",
    "                    print('Drift Detected at Batch ' + str(batch_index))\n",
    "                    drifts_detected.append(batch_index)\n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20)\n",
    "                    random_forest_distance.fit(current_window_x, current_window_y)\n",
    "                    reference_window = []\n",
    "                    reference_window.extend(current_window)\n",
    "        \n",
    "#                     reference_window = []\n",
    "#                     reference_window.extend(current_window)\n",
    "                current_window = []\n",
    "                batch = []\n",
    "                batch_index = batch_index + 1\n",
    "            \n",
    "print(batches)\n",
    "print(eval_accuracy_no_change)\n",
    "print(eval_accuracy_distance)\n",
    "print(drifts_detected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Agrawal Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Evaluation_Iteration = 50\n",
    "Batch_Size = 2500\n",
    "Drift_Batch = 20\n",
    "\n",
    "TPR_Total = []\n",
    "FPR_Total = []\n",
    "Delay_Total = []\n",
    "\n",
    "for i in range(Evaluation_Iteration):\n",
    "    print(\"Currently Working on Iteration: \" + str(i+1))\n",
    "    \n",
    "    batch_index = 0\n",
    "    batch = []\n",
    "\n",
    "    reference_window = []\n",
    "    current_window = []\n",
    "\n",
    "    drifts_detected = []\n",
    "    eval_accuracy_no_change = []\n",
    "    eval_accuracy_distance = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    Delay = 0\n",
    "    Delay_Stop_Count_Flag = False\n",
    "    \n",
    "    with open('Datasets/Synthetic_MOA_Agrawal_Abrupt.csv') as csv_file:\n",
    "#     with open('Datasets/Synthetic_MOA_RBF_Gradual.csv') as csv_file:\n",
    "        drifts_detected = []\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            batch.append(row)\n",
    "            if len(batch) == Batch_Size:\n",
    "                if batch_index == 0:\n",
    "                    reference_window.extend(batch)\n",
    "                    reference_window_df = pd.DataFrame(reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('9', axis=1)\n",
    "                    reference_window_y = reference_window_df['9']\n",
    "                \n",
    "                    random_forest_no_change = RandomForestClassifier(n_estimators=20,random_state=42)\n",
    "                    random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                    random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                else:\n",
    "                    current_window.extend(batch)\n",
    "                    \n",
    "                    sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                    \n",
    "#                     print(len(reference_window))\n",
    "#                     print(len(sample_reference_window))\n",
    "                    \n",
    "                    reference_window_df = pd.DataFrame(sample_reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('9', axis=1)\n",
    "                    \n",
    "                    current_window_df = pd.DataFrame(current_window)\n",
    "                    current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                    current_window_x = current_window_df.drop('9', axis=1)\n",
    "                    current_window_y = current_window_df['9']\n",
    "                    \n",
    "                    pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                    accuracy_no_change = accuracy_score(current_window_y, pred_no_change)\n",
    "                    eval_accuracy_no_change.append(accuracy_no_change)\n",
    "                    \n",
    "                    pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                    accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                    eval_accuracy_distance.append(accuracy_distance)\n",
    "                    \n",
    "                    drift_flag, drift_feature = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                    \n",
    "                    if batch_index == Drift_Batch:\n",
    "                        if drift_flag:\n",
    "                            Delay = Batch_Size - 1\n",
    "                            Delay_Stop_Count_Flag = True\n",
    "                        else:\n",
    "                            Delay = Batch_Size\n",
    "                    elif batch_index > Drift_Batch:\n",
    "                        if not(Delay_Stop_Count_Flag):\n",
    "                            if drift_flag:\n",
    "                                Delay = Delay + Batch_Size - 1\n",
    "                                Delay_Stop_Count_Flag = True\n",
    "                            else:\n",
    "                                Delay + Batch_Size\n",
    "                    \n",
    "                    if drift_flag:\n",
    "                        print('Drift Detected at Batch ' + str(batch_index))\n",
    "                        drifts_detected.append(batch_index)\n",
    "                        random_forest_distance = RandomForestClassifier(n_estimators=20)\n",
    "                        random_forest_distance.fit(current_window_x, current_window_y)\n",
    "                        reference_window = []\n",
    "                        reference_window.extend(current_window)\n",
    "                        \n",
    "                        if batch_index == Drift_Batch:\n",
    "                            TP = TP + 1\n",
    "                        else:\n",
    "                            FP = FP + 1\n",
    "                    else:\n",
    "                        reference_window.extend(current_window)\n",
    "                        if batch_index == Drift_Batch:\n",
    "                            FN = FN + 1\n",
    "                        else:\n",
    "                            TN = TN + 1\n",
    "#                     reference_window = []\n",
    "#                     reference_window.extend(current_window)\n",
    "                    current_window = []\n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                    \n",
    "        TPR = TP / (TP + FN)\n",
    "        FPR = FP / (FP + TN)\n",
    "        TPR_Total.append(TPR)\n",
    "        FPR_Total.append(FPR)\n",
    "        Delay_Total.append(Delay)\n",
    "\n",
    "mean_TPR = statistics.mean(TPR_Total)\n",
    "stdev_TPR = statistics.stdev(TPR_Total)\n",
    "mean_FPR = statistics.mean(FPR_Total)\n",
    "stdev_FPR = statistics.stdev(FPR_Total)\n",
    "mean_Delay = statistics.mean(Delay_Total)\n",
    "stdev_Delay = statistics.stdev(Delay_Total)\n",
    "\n",
    "print(\"TPR Total: \" + str(TPR_Total))\n",
    "print(\"FPR Total: \" + str(FPR_Total))\n",
    "print(\"Delay Total: \" + str(Delay_Total))\n",
    "print(\"TPR Mean: \" + str(mean_TPR))\n",
    "print(\"TPR Std: \" + str(stdev_TPR))\n",
    "print(\"FPR Mean: \" + str(mean_FPR))\n",
    "print(\"FPR Std: \" + str(stdev_FPR))\n",
    "print(\"Delay Mean: \" + str(mean_Delay))\n",
    "print(\"Delay Std: \" + str(stdev_Delay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1440\n",
    "\n",
    "batch_index = 0\n",
    "batch = []\n",
    "\n",
    "reference_window = []\n",
    "current_window = []\n",
    "\n",
    "drifts_detected = []\n",
    "eval_accuracy_no_change = []\n",
    "eval_accuracy_distance = []\n",
    "\n",
    "with open('Datasets/Real_ElecNorm.csv') as csv_file:\n",
    "    drifts_detected = []\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        batch.append(row)\n",
    "        if len(batch) == batch_size:\n",
    "            if batch_index == 0:\n",
    "                print(\"Currently working on batch \" + str(batch_index))\n",
    "                reference_window.extend(batch)\n",
    "                reference_window_df = pd.DataFrame(reference_window)\n",
    "                reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                reference_window_x = reference_window_df.drop('8', axis=1)\n",
    "                reference_window_x = reference_window_x.drop('0',axis=1)\n",
    "                reference_window_y = reference_window_df['8']\n",
    "                \n",
    "#                 print(\"Train Initial Classifier\")\n",
    "                random_forest_no_change = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                \n",
    "                random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                \n",
    "                pred_no_change = random_forest_no_change.predict(reference_window_x)\n",
    "                accuracy_no_change = accuracy_score(reference_window_y,pred_no_change)\n",
    "                eval_accuracy_no_change.append(accuracy_no_change)\n",
    "#                 print(\"Prediction Accuracy - No Change: \" + str(accuracy_no_change))\n",
    "                \n",
    "                pred_distance = random_forest_distance.predict(reference_window_x)\n",
    "                accuracy_distance = accuracy_score(reference_window_y, pred_distance)\n",
    "                eval_accuracy_distance.append(accuracy_distance)\n",
    "#                 print(\"Prediction Accuracy - Distance: \" + str(accuracy_distance))\n",
    "                \n",
    "                batch = []\n",
    "                batch_index = batch_index + 1\n",
    "            else:\n",
    "                print(\"Currently working on batch \" + str(batch_index))\n",
    "                print(len(reference_window))\n",
    "#                 reference_window_df = pd.DataFrame(sample_reference_window)\n",
    "                reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                reference_window_x = reference_window_df.drop('8', axis=1)\n",
    "                reference_window_x = reference_window_x.drop('0',axis=1)\n",
    "\n",
    "                current_window.extend(batch)\n",
    "                sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                \n",
    "                current_window_df = pd.DataFrame(current_window)\n",
    "                current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                current_window_x = current_window_df.drop('8', axis=1)\n",
    "                current_window_x = current_window_x.drop('0',axis=1)\n",
    "                current_window_y = current_window_df['8']\n",
    "                \n",
    "                pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                accuracy_no_change = accuracy_score(current_window_y,pred_no_change)\n",
    "                eval_accuracy_no_change.append(accuracy_no_change)\n",
    "#                 print(\"Prediction Accuracy - No Change: \" + str(accuracy_no_change))\n",
    "                \n",
    "                pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                eval_accuracy_distance.append(accuracy_distance)\n",
    "#                 print(\"Prediction Accuracy - Distance: \" + str(accuracy_distance))\n",
    "                \n",
    "#                 chunks.append(chunk_num)\n",
    "                \n",
    "                drift_flag, drift_feature = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                \n",
    "                if drift_flag:\n",
    "                    drifts_detected.append(batch_index)\n",
    "                    print('Drift Detected at batch ' + str(batch_index))\n",
    "                    reference_window = []\n",
    "                    reference_window.extend(current_window)\n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                    random_forest_distance.fit(current_window_x,current_window_y)\n",
    "                else:\n",
    "                    reference_window.extend(current_window)\n",
    "                    \n",
    "                current_window = []\n",
    "                batch = []\n",
    "                batch_index = batch_index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Phishing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "batch_index = 0\n",
    "batch = []\n",
    "\n",
    "reference_window = []\n",
    "current_window = []\n",
    "\n",
    "drifts_detected = []\n",
    "eval_accuracy_no_change = []\n",
    "eval_accuracy_distance = []\n",
    "\n",
    "with open('Datasets/Real_Phishing.csv') as csv_file:\n",
    "    drifts_detected = []\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        batch.append(row)\n",
    "        if len(batch) == batch_size:\n",
    "            if batch_index == 0:\n",
    "                print(\"Currently working on batch \" + str(batch_index))\n",
    "                reference_window.extend(batch)\n",
    "                reference_window_df = pd.DataFrame(reference_window)\n",
    "                reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                reference_window_x = reference_window_df.drop('46', axis=1)\n",
    "                reference_window_y = reference_window_df['46']\n",
    "                \n",
    "#                 print(\"Train Initial Classifier\")\n",
    "                random_forest_no_change = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                \n",
    "                random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                \n",
    "                pred_no_change = random_forest_no_change.predict(reference_window_x)\n",
    "                accuracy_no_change = accuracy_score(reference_window_y,pred_no_change)\n",
    "                eval_accuracy_no_change.append(accuracy_no_change)\n",
    "#                 print(\"Prediction Accuracy - No Change: \" + str(accuracy_no_change))\n",
    "                \n",
    "                pred_distance = random_forest_distance.predict(reference_window_x)\n",
    "                accuracy_distance = accuracy_score(reference_window_y, pred_distance)\n",
    "                eval_accuracy_distance.append(accuracy_distance)\n",
    "#                 print(\"Prediction Accuracy - Distance: \" + str(accuracy_distance))\n",
    "                \n",
    "                batch = []\n",
    "                batch_index = batch_index + 1\n",
    "            else:\n",
    "                print(\"Currently working on batch \" + str(batch_index))\n",
    "                current_window.extend(batch)\n",
    "                sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                print(len(reference_window))\n",
    "                print(len(sample_reference_window))\n",
    "                reference_window_df = pd.DataFrame(reference_window)\n",
    "                reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                reference_window_x = reference_window_df.drop('46', axis=1)\n",
    "\n",
    "                \n",
    "                sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                \n",
    "                current_window_df = pd.DataFrame(current_window)\n",
    "                current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                current_window_x = current_window_df.drop('46', axis=1)\n",
    "                current_window_y = current_window_df['46']\n",
    "                \n",
    "                pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                accuracy_no_change = accuracy_score(current_window_y,pred_no_change)\n",
    "                eval_accuracy_no_change.append(accuracy_no_change)\n",
    "#                 print(\"Prediction Accuracy - No Change: \" + str(accuracy_no_change))\n",
    "                \n",
    "                pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                eval_accuracy_distance.append(accuracy_distance)\n",
    "#                 print(\"Prediction Accuracy - Distance: \" + str(accuracy_distance))\n",
    "                \n",
    "#                 chunks.append(chunk_num)\n",
    "                \n",
    "                drift_flag, drift_feature = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                \n",
    "                if drift_flag:\n",
    "                    drifts_detected.append(batch_index)\n",
    "                    print('Drift Detected at batch ' + str(batch_index))\n",
    "#                     reference_window = []\n",
    "#                     reference_window.extend(current_window)\n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                    random_forest_distance.fit(current_window_x,current_window_y)\n",
    "#                 else:\n",
    "#                     reference_window.extend(current_window)\n",
    "                \n",
    "                reference_window = []\n",
    "                reference_window.extend(current_window)\n",
    "                current_window = []\n",
    "                batch = []\n",
    "                batch_index = batch_index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Significant_Wasserstein_Distance(df_1, df_2):\n",
    "    significant_flag = False\n",
    "    threshold = 0.05\n",
    "    feature_detected = []\n",
    "    feature_distance_example = []\n",
    "    \n",
    "    feature_names = df_1.columns\n",
    "    \n",
    "    for feature in feature_names:\n",
    "        df_1_feature_values = df_1[feature]\n",
    "        df_2_feature_values = df_2[feature]\n",
    "        \n",
    "        distance = stats.wasserstein_distance(df_1_feature_values, df_2_feature_values)\n",
    "        \n",
    "        if feature in ['29','30','31','32','33','34']:\n",
    "            print(\"Feature Name: \" + feature + \" Distance: \" + str(distance))\n",
    "            feature_distance_example.append(distance)\n",
    "\n",
    "        if distance > threshold:\n",
    "            print(\"Feature Name: \" + feature + \" Distance: \" + str(distance))\n",
    "            significant_flag = True\n",
    "            feature_detected.append(feature)\n",
    "        \n",
    "    return significant_flag, feature_detected, feature_distance_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Working on Iteration: 1\n",
      "Currently Working on Batch 0\n",
      "Currently Working on Batch 1\n",
      "Feature Name: 29 Distance: 0.009599999999999997\n",
      "Feature Name: 30 Distance: 0.008399999999999963\n",
      "Feature Name: 31 Distance: 0.017999999999999905\n",
      "Feature Name: 32 Distance: 0.0015999999999999348\n",
      "Feature Name: 33 Distance: 0.0016000000000000458\n",
      "Feature Name: 34 Distance: 0.0007999999999999119\n",
      "Currently Working on Batch 2\n",
      "Feature Name: 29 Distance: 0.0\n",
      "Feature Name: 30 Distance: 0.00759999999999994\n",
      "Feature Name: 31 Distance: 0.00759999999999994\n",
      "Feature Name: 32 Distance: 0.00039999999999995595\n",
      "Feature Name: 33 Distance: 0.007199999999999984\n",
      "Feature Name: 34 Distance: 0.005599999999999938\n",
      "Currently Working on Batch 3\n",
      "Feature Name: 29 Distance: 0.004400000000000015\n",
      "Feature Name: 30 Distance: 0.005599999999999938\n",
      "Feature Name: 31 Distance: 0.0011999999999999789\n",
      "Feature Name: 32 Distance: 0.0011999999999999789\n",
      "Feature Name: 33 Distance: 0.0015999999999999348\n",
      "Feature Name: 34 Distance: 0.0027999999999999137\n",
      "Currently Working on Batch 4\n",
      "Feature Name: 29 Distance: 0.0011999999999999789\n",
      "Feature Name: 30 Distance: 0.005199999999999982\n",
      "Feature Name: 31 Distance: 0.006399999999999961\n",
      "Feature Name: 32 Distance: 0.007600000000000051\n",
      "Feature Name: 33 Distance: 0.0011999999999999789\n",
      "Feature Name: 34 Distance: 0.0007999999999999119\n",
      "Currently Working on Batch 5\n",
      "Feature Name: 29 Distance: 0.011199999999999988\n",
      "Feature Name: 30 Distance: 0.0008000000000000229\n",
      "Feature Name: 31 Distance: 0.010399999999999965\n",
      "Feature Name: 32 Distance: 0.005199999999999982\n",
      "Feature Name: 33 Distance: 0.005599999999999938\n",
      "Feature Name: 34 Distance: 0.0035999999999999366\n",
      "Currently Working on Batch 6\n",
      "Feature Name: 29 Distance: 0.010799999999999976\n",
      "Feature Name: 30 Distance: 0.0031999999999999806\n",
      "Feature Name: 31 Distance: 0.013999999999999901\n",
      "Feature Name: 32 Distance: 0.0048000000000000265\n",
      "Feature Name: 33 Distance: 0.0040000000000000036\n",
      "Feature Name: 34 Distance: 0.005599999999999938\n",
      "Currently Working on Batch 7\n",
      "Feature Name: 29 Distance: 0.014400000000000024\n",
      "Feature Name: 30 Distance: 0.01079999999999992\n",
      "Feature Name: 31 Distance: 0.0252\n",
      "Feature Name: 32 Distance: 0.0044000000000000705\n",
      "Feature Name: 33 Distance: 0.0023999999999999577\n",
      "Feature Name: 34 Distance: 0.0047999999999999154\n",
      "Currently Working on Batch 8\n",
      "Feature Name: 29 Distance: 0.01200000000000001\n",
      "Feature Name: 30 Distance: 0.007199999999999984\n",
      "Feature Name: 31 Distance: 0.019199999999999995\n",
      "Feature Name: 32 Distance: 0.0011999999999999789\n",
      "Feature Name: 33 Distance: 0.006399999999999961\n",
      "Feature Name: 34 Distance: 0.0019999999999998908\n",
      "Currently Working on Batch 9\n",
      "Feature Name: 29 Distance: 0.058825974016399996\n",
      "Feature Name: 29 Distance: 0.058825974016399996\n",
      "Feature Name: 30 Distance: 0.00040000000000006697\n",
      "Feature Name: 31 Distance: 0.0252\n",
      "Feature Name: 32 Distance: 0.0008000000000000229\n",
      "Feature Name: 33 Distance: 0.005199999999999982\n",
      "Feature Name: 34 Distance: 0.07800000000000007\n",
      "Feature Name: 34 Distance: 0.07800000000000007\n",
      "Feature Name: 47 Distance: 0.07885931037280003\n",
      "Drift Detected at Batch 9\n",
      "Currently Working on Batch 10\n",
      "Feature Name: 29 Distance: 0.023594805190000002\n",
      "Feature Name: 30 Distance: 0.0028000000000000247\n",
      "Feature Name: 31 Distance: 0.02079999999999993\n",
      "Feature Name: 32 Distance: 0.0\n",
      "Feature Name: 33 Distance: 0.0028000000000000247\n",
      "Feature Name: 34 Distance: 0.005199999999999982\n",
      "Currently Working on Batch 11\n",
      "Feature Name: 29 Distance: 0.010109090905600008\n",
      "Feature Name: 30 Distance: 0.009600000000000053\n",
      "Feature Name: 31 Distance: 0.017199999999999993\n",
      "Feature Name: 32 Distance: 0.0016000000000000458\n",
      "Feature Name: 33 Distance: 0.0036000000000000476\n",
      "Feature Name: 34 Distance: 0.016800000000000037\n",
      "Currently Working on Batch 12\n",
      "Feature Name: 29 Distance: 0.008477922075200005\n",
      "Feature Name: 30 Distance: 0.014800000000000035\n",
      "Feature Name: 31 Distance: 0.025599999999999956\n",
      "Feature Name: 32 Distance: 0.0016000000000000458\n",
      "Feature Name: 33 Distance: 0.0020000000000000018\n",
      "Feature Name: 34 Distance: 0.010399999999999965\n",
      "Currently Working on Batch 13\n",
      "Feature Name: 29 Distance: 0.0140103896072\n",
      "Feature Name: 30 Distance: 0.010000000000000009\n",
      "Feature Name: 31 Distance: 0.025999999999999912\n",
      "Feature Name: 32 Distance: 0.0028000000000000247\n",
      "Feature Name: 33 Distance: 0.00880000000000003\n",
      "Feature Name: 34 Distance: 0.00759999999999994\n",
      "Currently Working on Batch 14\n",
      "Feature Name: 29 Distance: 0.004384415584\n",
      "Feature Name: 30 Distance: 0.0028000000000000247\n",
      "Feature Name: 31 Distance: 0.0020000000000000018\n",
      "Feature Name: 32 Distance: 0.0048000000000000265\n",
      "Feature Name: 33 Distance: 0.0040000000000000036\n",
      "Feature Name: 34 Distance: 0.010399999999999965\n",
      "Currently Working on Batch 15\n",
      "Feature Name: 29 Distance: 0.011054545451199995\n",
      "Feature Name: 30 Distance: 0.008399999999999963\n",
      "Feature Name: 31 Distance: 0.012799999999999923\n",
      "Feature Name: 32 Distance: 0.00039999999999995595\n",
      "Feature Name: 33 Distance: 0.006000000000000005\n",
      "Feature Name: 34 Distance: 0.016000000000000014\n",
      "Currently Working on Batch 16\n",
      "Feature Name: 29 Distance: 0.005480519482\n",
      "Feature Name: 30 Distance: 0.011600000000000055\n",
      "Feature Name: 31 Distance: 0.011599999999999944\n",
      "Feature Name: 32 Distance: 0.008000000000000007\n",
      "Feature Name: 33 Distance: 0.006000000000000005\n",
      "Feature Name: 34 Distance: 0.0043999999999999595\n",
      "Currently Working on Batch 17\n",
      "Feature Name: 29 Distance: 0.0174077922056\n",
      "Feature Name: 30 Distance: 0.0048000000000000265\n",
      "Feature Name: 31 Distance: 0.03199999999999992\n",
      "Feature Name: 32 Distance: 0.0028000000000000247\n",
      "Feature Name: 33 Distance: 0.0031999999999999806\n",
      "Feature Name: 34 Distance: 0.0011999999999999789\n"
     ]
    }
   ],
   "source": [
    "Evaluation_Iteration = 1\n",
    "Batch_Size = 2500\n",
    "# Drift_Batch = 9\n",
    "# Feature_Shift = [0,2,3,6,8,9,29,31,34,36,44,47]\n",
    "# Feature_Not_Shift = [1,4,5,7,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,30,32,33,35,37,38,39,40,41,42,43,45,46]\n",
    "# print(len(Feature_Shift))\n",
    "# print(len(Feature_Not_Shift))\n",
    "\n",
    "# TPR_Total = []\n",
    "# FPR_Total = []\n",
    "# Delay_Total = []\n",
    "# Batch_9_Interpretation_TPR_Total = []\n",
    "# Batch_9_Interpretation_FPR_Total = []\n",
    "\n",
    "\n",
    "for i in range(Evaluation_Iteration):\n",
    "    print(\"Currently Working on Iteration: \" + str(i+1))\n",
    "    \n",
    "    batch_index = 0\n",
    "    batch = []\n",
    "\n",
    "    reference_window = []\n",
    "    current_window = []\n",
    "\n",
    "    drifts_detected = []\n",
    "    eval_accuracy_no_change = []\n",
    "    eval_accuracy_distance = []\n",
    "    \n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    Delay = 0\n",
    "    Delay_Stop_Count_Flag = False\n",
    "    \n",
    "    with open('Datasets/Synthetic_Feature_Bank_Abrupt.csv') as csv_file:\n",
    "#     with open('Datasets/Synthetic_Feature_Bank_Gradual.csv') as csv_file:\n",
    "        drifts_detected = []\n",
    "        feature_distance = []\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            batch.append(row)\n",
    "            if len(batch) == Batch_Size:\n",
    "                print(\"Currently Working on Batch \" + str(batch_index))\n",
    "                if batch_index == 0:\n",
    "                    reference_window.extend(batch)\n",
    "                    reference_window_df = pd.DataFrame(reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('48', axis=1)\n",
    "                    reference_window_y = reference_window_df['48']\n",
    "                \n",
    "                    random_forest_no_change = RandomForestClassifier(n_estimators=20,random_state=42)\n",
    "                    random_forest_no_change.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    random_forest_distance = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "                    random_forest_distance.fit(reference_window_x, reference_window_y)\n",
    "                    \n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                else:\n",
    "                    current_window.extend(batch)\n",
    "                    \n",
    "                    sample_reference_window = random.sample(reference_window, len(current_window))\n",
    "                    \n",
    "#                     print(len(reference_window))\n",
    "#                     print(len(sample_reference_window))\n",
    "                    \n",
    "                    reference_window_df = pd.DataFrame(sample_reference_window)\n",
    "                    reference_window_df.columns = reference_window_df.columns.astype(str)\n",
    "                    reference_window_x = reference_window_df.drop('48', axis=1)\n",
    "                    \n",
    "                    current_window_df = pd.DataFrame(current_window)\n",
    "                    current_window_df.columns = current_window_df.columns.astype(str)\n",
    "                    current_window_x = current_window_df.drop('48', axis=1)\n",
    "                    current_window_y = current_window_df['48']\n",
    "                    \n",
    "                    pred_no_change = random_forest_no_change.predict(current_window_x)\n",
    "                    accuracy_no_change = accuracy_score(current_window_y, pred_no_change)\n",
    "                    eval_accuracy_no_change.append(accuracy_no_change)\n",
    "                    \n",
    "                    pred_distance = random_forest_distance.predict(current_window_x)\n",
    "                    accuracy_distance = accuracy_score(current_window_y, pred_distance)\n",
    "                    eval_accuracy_distance.append(accuracy_distance)\n",
    "                    \n",
    "                    drift_flag, drift_feature, feature_distance_example = Check_Significant_Wasserstein_Distance(reference_window_x, current_window_x)\n",
    "                    feature_distance.append(feature_distance_example)\n",
    "#                     if batch_index == Drift_Batch:\n",
    "#                         if drift_flag:\n",
    "#                             Delay = Batch_Size - 106\n",
    "#                             Delay_Stop_Count_Flag = True\n",
    "#                         else:\n",
    "#                             Delay = Batch_Size\n",
    "#                     elif batch_index > Drift_Batch:\n",
    "#                         if not(Delay_Stop_Count_Flag):\n",
    "#                             if drift_flag:\n",
    "#                                 Delay = Delay + Batch_Size - 106\n",
    "#                                 Delay_Stop_Count_Flag = True\n",
    "#                             else:\n",
    "#                                 Delay + Batch_Size\n",
    "                    \n",
    "                    if drift_flag:\n",
    "                        print('Drift Detected at Batch ' + str(batch_index))\n",
    "                        \n",
    "#                         shap_drift_flag, feature_interpretation_shift_detected = Check_Significant_Shap(random_forest_distance, reference_window_x, current_window_x)\n",
    "#                         eli5_flag, feature_interpretation_shift_detected = Check_Significant_ELI5_Local(random_forest_distance, reference_window_df, current_window_df)\n",
    "#                         PI_flag, feature_interpretation_shift_detected = Check_Significant_Permutation_Importance(random_forest_distance, reference_window_df, current_window_df)\n",
    "#                         print(feature_interpretation_shift_detected)\n",
    "#                         Interpretation_TP = 0\n",
    "#                         Interpretation_FP = 0\n",
    "#                         Interpretation_TN = 0\n",
    "#                         Interpretation_FN = 0\n",
    "#                         for feature in feature_interpretation_shift_detected:\n",
    "#                             if feature in Feature_Shift:\n",
    "#                                 Interpretation_TP = Interpretation_TP + 1\n",
    "#                             else:\n",
    "#                                 Interpretation_FP = Interpretation_FP + 1\n",
    "#                         for feature in Feature_Shift:\n",
    "#                             if feature not in feature_interpretation_shift_detected:\n",
    "#                                 Interpretation_FN = Interpretation_FN + 1\n",
    "#                         for featurea in Feature_Not_Shift:\n",
    "#                             if featurea not in feature_interpretation_shift_detected:\n",
    "#                                 Interpretation_TN = Interpretation_TN + 1\n",
    "#                         print(Interpretation_TP)\n",
    "#                         print(Interpretation_FN)\n",
    "#                         print(Interpretation_TN)\n",
    "#                         print(Interpretation_FP)\n",
    "#                         Interpretation_TPR = Interpretation_TP / (Interpretation_TP + Interpretation_FN)\n",
    "#                         Interpretation_FPR = Interpretation_FP / (Interpretation_FP + Interpretation_TN)\n",
    "#                         print(Interpretation_TPR)\n",
    "#                         print(Interpretation_FPR)\n",
    "#                         if batch_index == 9:\n",
    "#                             Batch_9_Interpretation_TPR_Total.append(Interpretation_TPR)\n",
    "#                             Batch_9_Interpretation_FPR_Total.append(Interpretation_FPR)\n",
    "                        \n",
    "                        drifts_detected.append(batch_index)\n",
    "                        random_forest_distance = RandomForestClassifier(n_estimators=20)\n",
    "                        random_forest_distance.fit(current_window_x, current_window_y)\n",
    "                        reference_window = []\n",
    "                        reference_window.extend(current_window)\n",
    "                        \n",
    "#                         if batch_index == Drift_Batch:\n",
    "#                             TP = TP + 1\n",
    "#                         else:\n",
    "#                             FP = FP + 1\n",
    "#                     else:\n",
    "#                         reference_window.extend(current_window)\n",
    "#                         if batch_index == Drift_Batch:\n",
    "#                             FN = FN + 1\n",
    "#                         else:\n",
    "#                             TN = TN + 1\n",
    "#                     reference_window = []\n",
    "#                     reference_window.extend(current_window)\n",
    "                    current_window = []\n",
    "                    batch = []\n",
    "                    batch_index = batch_index + 1\n",
    "                    \n",
    "#         TPR = TP / (TP + FN)\n",
    "#         FPR = FP / (FP + TN)\n",
    "#         TPR_Total.append(TPR)\n",
    "#         FPR_Total.append(FPR)\n",
    "#         Delay_Total.append(Delay)\n",
    "\n",
    "# mean_TPR = statistics.mean(TPR_Total)\n",
    "# stdev_TPR = statistics.stdev(TPR_Total)\n",
    "# mean_FPR = statistics.mean(FPR_Total)\n",
    "# stdev_FPR = statistics.stdev(FPR_Total)\n",
    "# mean_Delay = statistics.mean(Delay_Total)\n",
    "# stdev_Delay = statistics.stdev(Delay_Total)\n",
    "\n",
    "# print(\"TPR Total: \" + str(TPR_Total))\n",
    "# print(\"FPR Total: \" + str(FPR_Total))\n",
    "# print(\"Delay Total: \" + str(Delay_Total))\n",
    "# print(\"TPR Mean: \" + str(mean_TPR))\n",
    "# print(\"TPR Std: \" + str(stdev_TPR))\n",
    "# print(\"FPR Mean: \" + str(mean_FPR))\n",
    "# print(\"FPR Std: \" + str(stdev_FPR))\n",
    "# print(\"Delay Mean: \" + str(mean_Delay))\n",
    "# print(\"Delay Std: \" + str(stdev_Delay))\n",
    "\n",
    "# print(\"Feature Drift Interpretation\")\n",
    "# mean_Batch_9_Interpretation_TPR = statistics.mean(Batch_9_Interpretation_TPR_Total)\n",
    "# stdev_Batch_9_Interpretation_TPR = statistics.stdev(Batch_9_Interpretation_TPR_Total)\n",
    "# mean_Batch_9_Interpretation_FPR = statistics.mean(Batch_9_Interpretation_FPR_Total)\n",
    "# stdev_Batch_9_Interpretation_FPR = statistics.stdev(Batch_9_Interpretation_FPR_Total)\n",
    "# print(\"TPR Batch 9 Total: \" + str(Batch_9_Interpretation_TPR_Total))\n",
    "# print(\"FPR Batch 9 Total: \" + str(Batch_9_Interpretation_FPR_Total))\n",
    "# print(\"TPR Batch 9 Interpretation Mean: \" + str(mean_Batch_9_Interpretation_TPR))\n",
    "# print(\"TPR Batch 9 Interpretation Std: \" + str(stdev_Batch_9_Interpretation_TPR))\n",
    "# print(\"FPR Batch 9 Interpretation Mean: \" + str(mean_Batch_9_Interpretation_FPR))\n",
    "# print(\"FPR Batch 9 Interpretation Std: \" + str(stdev_Batch_9_Interpretation_FPR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 29: [0.009599999999999997, 0.0, 0.004400000000000015, 0.0011999999999999789, 0.011199999999999988, 0.010799999999999976, 0.014400000000000024, 0.01200000000000001, 0.058825974016399996, 0.023594805190000002, 0.010109090905600008, 0.008477922075200005, 0.0140103896072, 0.004384415584, 0.011054545451199995, 0.005480519482, 0.0174077922056]\n",
      "Feature 30: [0.008399999999999963, 0.00759999999999994, 0.005599999999999938, 0.005199999999999982, 0.0008000000000000229, 0.0031999999999999806, 0.01079999999999992, 0.007199999999999984, 0.00040000000000006697, 0.0028000000000000247, 0.009600000000000053, 0.014800000000000035, 0.010000000000000009, 0.0028000000000000247, 0.008399999999999963, 0.011600000000000055, 0.0048000000000000265]\n",
      "Featire 31: [0.017999999999999905, 0.00759999999999994, 0.0011999999999999789, 0.006399999999999961, 0.010399999999999965, 0.013999999999999901, 0.0252, 0.019199999999999995, 0.0252, 0.02079999999999993, 0.017199999999999993, 0.025599999999999956, 0.025999999999999912, 0.0020000000000000018, 0.012799999999999923, 0.011599999999999944, 0.03199999999999992]\n",
      "Feature 32: [0.0015999999999999348, 0.00039999999999995595, 0.0011999999999999789, 0.007600000000000051, 0.005199999999999982, 0.0048000000000000265, 0.0044000000000000705, 0.0011999999999999789, 0.0008000000000000229, 0.0, 0.0016000000000000458, 0.0016000000000000458, 0.0028000000000000247, 0.0048000000000000265, 0.00039999999999995595, 0.008000000000000007, 0.0028000000000000247]\n",
      "Feature 33: [0.0016000000000000458, 0.007199999999999984, 0.0015999999999999348, 0.0011999999999999789, 0.005599999999999938, 0.0040000000000000036, 0.0023999999999999577, 0.006399999999999961, 0.005199999999999982, 0.0028000000000000247, 0.0036000000000000476, 0.0020000000000000018, 0.00880000000000003, 0.0040000000000000036, 0.006000000000000005, 0.006000000000000005, 0.0031999999999999806]\n",
      "Feature 34: [0.0007999999999999119, 0.005599999999999938, 0.0027999999999999137, 0.0007999999999999119, 0.0035999999999999366, 0.005599999999999938, 0.0047999999999999154, 0.0019999999999998908, 0.07800000000000007, 0.005199999999999982, 0.016800000000000037, 0.010399999999999965, 0.00759999999999994, 0.010399999999999965, 0.016000000000000014, 0.0043999999999999595, 0.0011999999999999789]\n"
     ]
    }
   ],
   "source": [
    "# print(len(feature_distance))\n",
    "feature_29_distance = []\n",
    "feature_30_distance = []\n",
    "feature_31_distance = []\n",
    "feature_32_distance = []\n",
    "feature_33_distance = []\n",
    "feature_34_distance = []\n",
    "\n",
    "for f in feature_distance:\n",
    "    feature_29_distance.append(f[0])\n",
    "    feature_30_distance.append(f[1])\n",
    "    feature_31_distance.append(f[2])\n",
    "    feature_32_distance.append(f[3])\n",
    "    feature_33_distance.append(f[4])\n",
    "    feature_34_distance.append(f[5])\n",
    "\n",
    "print(\"Feature 29: \" + str(feature_29_distance))\n",
    "print(\"Feature 30: \" + str(feature_30_distance))\n",
    "print(\"Featire 31: \" + str(feature_31_distance))\n",
    "print(\"Feature 32: \" + str(feature_32_distance))\n",
    "print(\"Feature 33: \" + str(feature_33_distance))\n",
    "print(\"Feature 34: \" + str(feature_34_distance))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
